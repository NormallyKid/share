\\\\\\\\\\\\\\\
intro
infor gathering -> vul assessment -> lateral move -> POC ->post engament
-identifying assessts -discovering Hidden Information - Analysing the Attack Surface -Gathering Intelligence
\\type of reconnaissance 
-active port scan/vulassessment/Network Mapping/Banner grabbing/ OS fingerprinting/Service enum/Web Spidering/
-passive: search engine Queries/ Whois Lookups/ DNS/ Web Archive Analysis/ Social Media Analysis/Code repositories
\\Whois
query and response protocol designed to access db that store infor about registered internet resource.it also provide detail about IP, blocks and autonomous systems
$whois <domainname>
\\GDPR general data protection regulation 
Whois are underway to strike a balance through initiatives like the Registration Data Access Protocol (RDAP) which offers a more granular and privacy-conscious approach to accessing domain registration data
-it identifying key personal -Discovering Network Infrastructure -Historical Data Analysis
https://whoisfreaks.com/
\\Utilising Whois
\\scenarios 1: Phishing Investigation
- a security analyst investigation the email and begins by performing a WHOIS lookup on the domain linked in the email
it reveals :-Registration Data -Registrant -Name Servers: the name servers are associated with a known bulletproof hosting privider often used for malicious activities
- this combine of factors raises significant red flags for the analyst. the recent registration date,...
\\Scenario 2: Malware Analysis
during analysing a new strain of malware that has infected several sys within a network the malware comm with a remote server to receive commands and exfiltrate stolen data. to gain insight into the threat actor infra. 
the researcher perform a WHOIS lookup on the domain associated with the C2 server
it reveal: -Registrant -Locaion -Registrar
-based on this infor the researcher concludes that C2 server is likely hosted on a compromised or bulletproof server ->identify the hosting provider and notify them of the malicious activity 
\\Scenario 3: Threat Intelligence repositories
- track the activities of a sophisticated threat actor group known for targeting financial institutions, analysts gather WHOIS data on multiple domains associated with the group's past campaigns to compile a comprehensive threat intelligencereport
it reveal: -registration dates -registrants -Name Servers -Takedown History
-> allow analysts to create a detailed profile of te threat actor's tactics, techniques, procedures(TTPs) the report includes indicators of compromise(IOCs)
-> detect block future attack
\\Using WHOIS
$sudo apt update -> $sudo api install whois -y
$whois facebook.com
-> domain registration -Domain Owner - Domain status -Nameserver suggest companay manage it DNS infra
\\\\\\\\\=================================
DNS
dns acts as the internet GPS, guiding from memorable landmarks to precise number coordinates.
\\how DNS works
1.comp query for DNS ->check cache -> DNS resolver. (ISP provider) 
2. DNS Resolver Checks its Map (Recursive Lookup): resolver also has a cache, and if it doesn;t find the IP addr there, it starts a journey through the DNS hierarchy. -> asking root name server (internet library)
3. Root Name Server Points the Way:-> doesnt know the exact addr but knows who doesn -> top-level (TLD) name server responsible for domains ending (eg,..com,.org) it points the resolver in the right direction.
4. TLD name server narrows it down(like a regional map) it know which authoritative name server is responsible for the specific domain you're looking for(Eg:exmple.com)
5.Authoritative name server delivers the addr: final stop, it's like the street address of the website you want. it hold the correct IP addr and sends it back to the resolver.
6 the DNS Resolver Returns the information: the resolver receives IP addr and gives it to comp it also remembers it for a while (cache it) 
7. comp connects -> start browsing
\\The Host File
is a simple text file used to map hostnames to IP addr, providing a manual method of domain name resolution that bypass the DNS process. while DNS automates the translation of domain names to IP addr, the hosts file allows for direct, local overrides. this can be particularly useful for deve troubleshotting or blocking website
-The hosts file is located in C:\Windows\System32\drivers\etc\hosts on Windows and in /etc/hosts on linux and MacOS, each line in the file follows the format:
<IP address>                <Hostname> [<Alias>...] 
127.0.0.1                   localhost
192.168.1.10                devserver.local
-commonly uses include redirecting a domain to a local server for development:
-testing connectivity by specifying an IP addr
192.168.1.20                testserver.local
-blocking unwanted websites by redirecting their domains to a non-existent IP addr
0.0.0.0                     unwanted-site.com
\\Replay reconnaissance
-think of the DNS process as a relay race. comp starts with the dn and passes it along the resolver, the resolve then passes the request to the root server, the TLD server, and finally, the authoritative server. each one getting closer to the destination. once the IP addr is found, it relayed back down the chain to your comp, allowing to access the website.
\\Key DNS Concepts
-in dns, a zone is a distinct part of the domain namespace that a specific entity or administrator manages.think of it as a virtual container for a set of domain names. 
EX example.com and all its subdomains(like mail.example.com or blog.example.com) would typically belong to the same DNS zone
-the zone file, a text file residing on a DNS server, defines the resource records (discussed bwlow) withing this zone, providing crucial infor for translating domain name in to IP addr
-to illustrate, here;s a simplified example of what a zone file, for Example.com might look like:
$TTL 3600 ; Default Time-To-Live (1 hour)
@       IN SOA   ns1.example.com. admin.example.com. (
                2024060401 ; Serial number (YYYYMMDDNN)
                3600       ; Refresh interval
                900        ; Retry interval
                604800     ; Expire time
                86400 )    ; Minimum TTL

@       IN NS    ns1.example.com.
@       IN NS    ns2.example.com.
@       IN MX 10 mail.example.com.
www     IN A     192.0.2.1
mail    IN A     198.51.100.1
ftp     IN CNAME www.example.com.
-this file defines the authoritative name servers (NS record), mail server (MX record), and IP addr (A record) for various hosts within the example.com domain.
-DNS  server store various resource records, each serving a specific purpose in the domain name resolution process 
DNS Concept	Description	Example
Domain Name	A human-readable label for a website or other internet resource.	www.example.com
IP Address	A unique numerical identifier assigned to each device connected to the internet.	192.0.2.1
DNS Resolver	A server that translates domain names into IP addresses.	Your ISP's DNS server or public resolvers like Google DNS (8.8.8.8)
Root Name Server	The top-level servers in the DNS hierarchy.	There are 13 root servers worldwide, named A-M: a.root-servers.net
TLD Name Server	Servers responsible for specific top-level domains (e.g., .com, .org).	Verisign for .com, PIR for .org
Authoritative Name Server	The server that holds the actual IP address for a domain.	Often managed by hosting providers or domain registrars.
DNS Record Types	Different types of information stored in DNS.	A, AAAA, CNAME, MX, NS, TXT, etc.

-fundamental concepts of DNS, dive deeper into the building blocks of DNS infor- the various record types. these record store diff types of data associated with domain names. each serving a specific purpose

ecord Type	Full Name	Description	Zone File Example
A	Address Record	Maps a hostname to its IPv4 address.	www.example.com. IN A 192.0.2.1
AAAA	IPv6 Address Record	Maps a hostname to its IPv6 address.	www.example.com. IN AAAA 2001:db8:85a3::8a2e:370:7334
CNAME	Canonical Name Record	Creates an alias for a hostname, pointing it to another hostname.	blog.example.com. IN CNAME webserver.example.net.
MX	Mail Exchange Record	Specifies the mail server(s) responsible for handling email for the domain.	example.com. IN MX 10 mail.example.com.
NS	Name Server Record	Delegates a DNS zone to a specific authoritative name server.	example.com. IN NS ns1.example.com.
TXT	Text Record	Stores arbitrary text information, often used for domain verification or security policies.	example.com. IN TXT "v=spf1 mx -all" (SPF record)
SOA	Start of Authority Record	Specifies administrative information about a DNS zone, including the primary name server, responsible person's email, and other parameters.	example.com. IN SOA ns1.example.com. admin.example.com. 2024060301 10800 3600 604800 86400
SRV	Service Record	Defines the hostname and port number for specific services.	_sip._udp.example.com. IN SRV 10 5 5060 sipserver.example.com.
PTR	Pointer Record	Used for reverse DNS lookups, mapping an IP address to a hostname.	1.2.0.192.in-addr.arpa. IN PTR www.example.com.
-the IN stands for "Internet" it's a class field in DNS records that specifies the protocol family. in most cases, you;ll see "IN" used, as it denotes the internet protocol suite (IP used for most domain names.
-other class values exist (EG: CH for Chaosnet, HS for hesiod)) but are rarely used in morderm DNS config
-in essence"IN" is simply a convention that indicates that the record applies to the standard internet protocols we use today. while it might seem like an extra detail. understanding its meaning provides a deeper understanding of DNS record structure
\\Why DNS matters for web reconnaissance
it's a critical component of a target's infras that can be leveraged to uncover vul and gain access during a pentest 
+uncovering assets:reveak a wealth of infor, subdomain, mail server, anem name server record, EX: a cname record pointing to and outdated server (dev.example.com CNAME oldserver.example.net) could leak to a vulnerable system
+Mapping the network infras: create comprehensive map of the target network infrs by analysing DNS data. EX identifying the name server (NS records) which reveal hosting provider used, while an A record for loadbalancer.example.com can pinpoint a load balancer. ->understand diff system are connected, traffic flow, pinpoint potential choke points or weaknesses that could be exploited during a penetration test
+Monitoring for changes: continously monitoring DNS records can reveal changes in the target's infra over time. EX the sudden appearance of a new subdomain (VPN.example.com)might indicate a new entry point into the network, while a TXT record containing a value like _1password=... strongly suggests the org is using 1passowrd, which could be leveraged for social engineering attacks or targetd phishing campaigns.
\\\\\\\\\=================================
Digging DNS
having established a solid understanding of DNS fundamentals and its various record types, lets now transition to the practical. this section will explore the tools and techniques for leveraging DNS for WEB reconnaissance
\\DNS Tools
involves utilizing specialized tools designed to query DNS servers and extract valuable infor. here are some of the most popular and versatile tools in the arsenal of web recon procedures

Tool	Key Features	Use Cases
dig	Versatile DNS lookup tool that supports various query types (A, MX, NS, TXT, etc.) and detailed output.	Manual DNS queries, zone transfers (if allowed), troubleshooting DNS issues, and in-depth analysis of DNS records.
nslookup	Simpler DNS lookup tool, primarily for A, AAAA, and MX records.	Basic DNS queries, quick checks of domain resolution and mail server records.
host	Streamlined DNS lookup tool with concise output.	Quick checks of A, AAAA, and MX records.
dnsenum	Automated DNS enumeration tool, dictionary attacks, brute-forcing, zone transfers (if allowed).	Discovering subdomains and gathering DNS information efficiently.
fierce	DNS reconnaissance and subdomain enumeration tool with recursive search and wildcard detection.	User-friendly interface for DNS reconnaissance, identifying subdomains and potential targets.
dnsrecon	Combines multiple DNS reconnaissance techniques and supports various output formats.	Comprehensive DNS enumeration, identifying subdomains, and gathering DNS records for further analysis.
theHarvester	OSINT tool that gathers information from various sources, including DNS records (email addresses).	Collecting email addresses, employee information, and other data associated with a domain from multiple sources.
Online DNS Lookup Services	User-friendly interfaces for performing DNS lookups.	Quick and easy DNS lookups, convenient when command-line tools are not available, checking for domain availability or basic information

-dig command (Domain Information Groper) is a versatile and powerful utility for querying DNS servers and retriving various types of DNS records, its flexibility and detailed and customizable output make it a go-to choice
\\
Command	Description
dig domain.com	Performs a default A record lookup for the domain.
dig domain.com A	Retrieves the IPv4 address (A record) associated with the domain.
dig domain.com AAAA	Retrieves the IPv6 address (AAAA record) associated with the domain.
dig domain.com MX	Finds the mail servers (MX records) responsible for the domain.
dig domain.com NS	Identifies the authoritative name servers for the domain.
dig domain.com TXT	Retrieves any TXT records associated with the domain.
dig domain.com CNAME	Retrieves the canonical name (CNAME) record for the domain.
dig domain.com SOA	Retrieves the start of authority (SOA) record for the domain.
dig @1.1.1.1 domain.com	Specifies a specific name server to query; in this case 1.1.1.1
dig +trace domain.com	Shows the full path of DNS resolution.
dig -x 192.168.1.1	Performs a reverse lookup on the IP address 192.168.1.1 to find the associated host name. You may need to specify a name server.
dig +short domain.com	Provides a short, concise answer to the query.
dig +noall +answer domain.com	Displays only the answer section of the query output.
dig domain.com ANY	Retrieves all available DNS records for the domain (Note: Many DNS servers ignore ANY queries to reduce load and prevent abuse, as per RFC 8482).
-caution: some servers can detect and block excessive DNS queries. use caution and respect rate limits always obtain permission before performing extensive DNS reconnaissance on the target
\\Groping DNS
$dig google.com
-result might be broken down into four key sections:
1.Header
-;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 16449 -> this indicates the type of query, the succesful status (NOERROR), and unique id for specific query
-;; flags: qr rd ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0: this describes the flags in the DNS header:
    +qr: Query Response flag - indicates this is a response.
    +rd: Recursive desired flag - means recursive was requested
    +ad: Authentic Data flag - means the resolver considers the data authentic.
    +the remaining numbers indicate the number of entries in each section of the DNS response:1 question, 1 answer, 0 authority records, and - additional records.
-;; WARNING: recursion requested but not available: This indicates that recursion was requested, but the server does not support it.
2. Question section
-google.com. IN A: this ........................
- an OPT pseudosection can sometimes exist in a dig query. this is dueto extention mechanisms for DNS (EDNS), which allows for additional features such as larger message sizes and DNS security extension(DNSSEC) support.
- get only answer 
$dig +short Hackthebox.com
<test>Which IP address maps to inlanefreight.com?
dig <url>
<test>Which domain is returned when querying the PTR record for 134.209.24.248?

<test>What is the full domain returned when you query the mail records for facebook.com?
dig <url> mx
\\\\\=====================================
subdomains
there a potential infor besite domain ->subdomain these are extensions of the main domain, often created to organise and separate different sections or functionalities of a website. 
EX: a company might use blog.example.com for its blog, shop.example.com for its online store, or mail.example.com for its email services
\\why it important
subdomains often host valuable infor and resources that aren't directly linked from the main website. this can include:
+development and staging environment: tesst dev might contain vulnerabilities or expose sensitive infor
+Hidden login portals
+legacy application: 
+sensitive information: it inadvertently expose confidential documents, internal data, or configuration files that could be valuable to attackers
\\Subdomain enumeration
-it is the process of systematically identifying and listing these subdomains, from a DNS perspective, subdomains are typically represented by A for AAAA for IPv6 record which map the subdomain name to its IP additonally.
CNAME records might be used to create aliases for sub, pointing them to other domains or sub. there are few main approaches to subdomain enum:
1. Active Subdomain enumeration
-directly interacting with the target domain DNS server to uncover sub. one method is attempting a DNS zone transfer, where a misconfigured server might inadvertently leak a complete list of sub. however, due to tightened security measures, this is rarely successful
-brute-force enumeration, which involves systematically testing a list of potential sub names agaisnt the target domain. Tools like dnsenum, ffuf, and gobuster
can automate this process, using wordlists of common sub names or custom-generated lists based on specific partterns
2. Passive subdomain enumeration
-one valuable resource is Certificate Transparency (CT) logs, public repositories of SSL/TLS cert. these cert often include a list of associated subdomains in their subject alternative name (SAN) field.
-search engine . (site: ,...)
\\\=========================
subdomain bruteforcing
1. wordlist selection: + General-purpose: broad range of common subdomain names (e.g:,dev,staging,blog,mail,admin,test) this useful when dont know the target;s naming conventions
+targetd: Focused on specific industries, technologies, or naming patterns relevant to the target. this approach is more efficient and reduces the chances of false positives.
+custom: you can create your own wordlist based on specific keywords, patterns, or intelligence gathered from other sources.
2.iteration and querying: A script or tool iterates through the wordlist, appending each word or phrase to the main domain (e.g example.com) to create potential subdomain names (e.g dev.ex.com, staging.ex.com)
3. DNS lookup: check if it resolves to an IP
4. filtering and validation: if a sub resolves successfuly, it's added to a list of valid subdomains. further validation steps might be taken to confirm the subdomain;s existence and functionality (e.g by attempting to access it through a web browser)

Tool	Description
dnsenum	Comprehensive DNS enumeration tool that supports dictionary and brute-force attacks for discovering subdomains.
fierce	User-friendly tool for recursive subdomain discovery, featuring wildcard detection and an easy-to-use interface.
dnsrecon	Versatile tool that combines multiple DNS reconnaissance techniques and offers customisable output formats.
amass	Actively maintained tool focused on subdomain discovery, known for its integration with other tools and extensive data sources.
assetfinder	Simple yet effective tool for finding subdomains using various techniques, ideal for quick and lightweight scans.
puredns	Powerful and flexible DNS brute-forcing tool, capable of resolving and filtering results effectively.

\\dnsenum
+dns record enumeration:can retrieve various DNS records, including A, AAAA, NS, MX, and TXT records, 
+Zone transfer attempts: the tool automatically attempts zone transfers from discovered name servers, while most servers are configurated
+subdomain brute-forcing: this involves systematically testing potential subdomain name against the target domain to identify valid One
+Google scraping: the tool can scrape google search results to find add sub that might not be listed in DNS records directly
+Reverse lookup:can perform reverse DNS lookup to identify domains associated with a given IP addr, potentially revealing other web hosted on the same server
+Whois lookups: perform WHOIS queries to gather infor about domain ownership and registration details
$dnsenum --enum inlanefreight.com -f /usr/share/seclists/Discovery/DNS/subdomains-top1million-110000.txt -r
-f indicate the path to the Seclists wordlist we'll use for brute-forcing. Adjust the path if your Seclists installation is different
-r enables recursive subdomain brute-forcing, if it found sub -> it will continue find sub of it
<test>Using the known subdomains for inlanefreight.com (www, ns1, ns2, ns3, blog, support, customer), find any missing subdomains by brute-forcing possible domain names. Provide your answer with the complete subdomain, e.g., www.inlanefreight.com.
$dnsenum --enum inlanefreight.com -f /usr/share/seclists/Discovery/DNS/subdomains-top1million-110000.txt -r

\\=========================================
DNS Zone transfer
while bruteforcing theres less invasive and potentially more efficient method for uncovering sub -DNS zone transfers, this mechanism, designed for replicating DNS records between name servers, can inadvertently become a goldmine of information for prying eyes if misconfigured
\\Whhat is zone transfer
A DNS zone transfer is essentially a whole sale copy of all DNS records within a zone(adomain and its sub)
from one name server to another. this process is essential for maintaining consistency and redundancy across DNS servers.hwer if not adequately secured, unauthorised parties can download the entire zone file, revealing a complete list of subdomains, their associated IP addresses and other sensitive DNS data
-secondaryServer                    PrimaryServer
            FXFR Request (zone transfer)-->
        <-- SOA Record (start of authority) :contain vital infor about the zone, it serial num, which help the secondary server determine if its zone data is current
        loop        [transfer]     :transfer all the DNS records in ther zone to 2nd zone, A,AAAA,MX,Cname,NS,... other sub, mailserver,..
        <--         DNS record
        <--          Zone transfer complete
                ACK (Acknowledgement) -->

secondaryServer                     PrimaryServer
\\The Zone Transfer Vulnerability
while zone transfer are essential for legit DNS management, a misconfigured DNS server can transform this process into a significant security vulnerability.the core issues lies in the access controls governing who can initiate a zone transfer
-the infor gleaned from an unauthorised zone transfer can be invaluable to an attacker, it reveals a comprehensive map of the target's DNS infrastructure, inlcuding:
+subdomains: a complete list of subdomains + IP addr +name server records detail about the authoritative name servers for the domain, revealing the hosting provider and ptential misconfigurations.
+IP addr +name server records
\\Remediation
modern DNS svr are typically config to allow zone transfer only to trusted secondary svr. ensure zone data remains confidential
but smt human made ->wy a zonetransfer with proper authorisation remains a valuable reconnaissance technique. even if unsuccesful, the attempt can reveal information about the DNS server's config and security posture
\\Exploiting Zone transfers
$dig axfr @nsztm1.digi.ninja zonetransfer.me 
this command instruct dig to request a full zone transfer (axfr) from dns svr responsible for zonetransfer.me if the svr is misconfig and allow transfer. we migh reveive a complete list of DNS records for the domain including all sub
$dig axfr @nsztm1.digi.ninja zonetransfer.me 
zone transfer is a service specifically setup to demo the risk of zone transfer so that the dig command will return the axfr
<test>After performing a zone transfer for the domain inlanefreight.htb on the target system, how many DNS records are retrieved from the target system's name server? Provide your answer as an integer, e.g, 123.
$dig axfr <url> @<ip>
<test> Within the zone record transferred above, find the ip address for ftp.admin.inlanefreight.htb. Respond only with the IP address, eg 127.0.0.1
$dig axfr <url> @<ip> | grep "ftp.admin.inlanefreight.htb"
<test>Within the same zone record, identify the largest IP address allocated within the 10.10.200 IP range. Respond with the full IP address, eg 10.10.200.1
$dig axfr <url> @<ip> | grep "10.10.200"
\\=====================================
Virtual hosts
once DNS direct traffic to the correct server, the web svr config becomes crucial in determining how the incoming requests are hadled/
web svr like apache, nginx, or IIS are designed to host multiple websites or app on single svr. they achive this through virtual hosting, which allows them to differentiate betw domains, subdomains, or even sepparate websites with distinct contents
\\How virtual hosts work 
virtual hosting is the ability of web svr to distinguish betw multiple websites or app sharing the same IP addr
VHostsL are config within a web svr that allow multiple  websites or app to be hosted on a single svr. they can be associated with top-level domains (e.g example.com) or submains (e.g dev.example.com)each virtual host can have its own separate config, enabling precise control over how request are handled
-if a virtual host does not have a DNS record, you can still access it by modifyingthe hosts file on local machine
-the host file allows you to map a domain name to and IP addr manually bypassing DNS resolution
-website often have sub that are not public wont appear in DNS records, these sub are only accessible internally or through specific config.
-VHost fuzzing is a technique to discover public and non-public subdomains and VHosts by testing various hostnames against a known IP addr.
-Virtual hosts can also be configured to use different domains, not just subdomains. EX:
<apacheconf>
# Example of name-based virtual host configuration in Apache
<VirtualHost *:80>
    ServerName www.example1.com
    DocumentRoot /var/www/example1
</VirtualHost>

<VirtualHost *:80>
    ServerName www.example2.org
    DocumentRoot /var/www/example2
</VirtualHost>

<VirtualHost *:80>
    ServerName www.another-example.net
    DocumentRoot /var/www/another-example
</VirtualHost>
--------------------------------------
3 website are distinct domains hosted on the same svr, the web svr uses the Host header to serve the appropriate content based on the requested domain name
\\Server VHost Lookup
the following illustrates the process of how a web svr determines the correct content to serve based on the Host header:
1.browser request a web -> 2. host header reveal the domain ->3Web svr determines the virtual host -> 4. serving the right content
-in essence the Host header functions as a switch, enabling the web svr to dynamically determine which web to serve based on the domain name requested by the browser
\\Type of Virtual hosting
+Name-based virtual hosting +IP-based Virtual hosting +port-based virtual hosting

curl -x http://127.0.0.1:8080 https://example.com
-x specifies the proxy (127.0.0.1:8080)

https://example.com is using port 443

Curl will send a CONNECT request to the proxy, which then opens a tunnel to port 443.
\\Virtual Host Discovery Tools
while manual analysis of HTTP header and reverse DNS lookups can be effective, specilised virtual host discovery tools automate and streamline the process, making it more efficient and comprehensive. these tools employ various techniques to probe the target server and uncover potential virtual hosts
-tool are available to aid in the discovery of virtual hosts:
tool                Description                         Features
gobuster            dir/file brute-forcing also effective virtual host
feroxbuster         similar to gobuster but rust-based implementation (speed and flexibility)
ffuf                fuzing the host header
Tool	Description	Features
gobuster	A multi-purpose tool often used for directory/file brute-forcing, but also effective for virtual host discovery.	Fast, supports multiple HTTP methods, can use custom wordlists.
Feroxbuster	Similar to Gobuster, but with a Rust-based implementation, known for its speed and flexibility.	Supports recursion, wildcard discovery, and various filters.
ffuf	Another fast web fuzzer that can be used for virtual host discovery by fuzzing the Host header.	Customizable wordlist input and filtering options.
\\Gobuster
1. target identification: identify the target web server's IP addr. ths can be done throgh DNS lookups or other reconnaissance techniques.
2. Wordlist preparation: a wordlist containing potential virtual host names. can use pre-compiled wordlist, sich as Seclists, or create a custom one based on targets industry, namin conventionsm or other relevant onfor
gobuster bruteforcing 
$ gobuster vhost -u https://<targetIP> -w <wordlist_file> --append-domain
<-u specifies the target URL -W specifies wordlist file --append-domain flag appends the base domain to each word in the wordlist
<-t to increase the number of threads for faster scanning -k ignore SSL/TLS cert errors -o save the output to a file for later analysis
<test>Brute-force vhosts on the target system. What is the full subdomain that is prefixed with "web"? Answer using the full domain, e.g. "x.inlanefreight.htb"
gobuster vhost -u https://<ip> -w /usr/share/seclists/Discovery/DNS/subdomains-top1million-110000.txt --append-domain
<test>Brute-force vhosts on the target system. What is the full subdomain that is prefixed with "vm"? Answer using the full domain, e.g. "x.inlanefreight.htb"
sudo sh -c "echo '<ip> inlanefreight.htb' >> /etc/hosts" 94.237.57.211:42008
$gobuster vhost -u http://inlanefreigh.htb:<port>/ -w /usr/share/sec,...
<test> Brute-force vhosts on the target system. What is the full subdomain that is prefixed with "br"? Answer using the full domain, e.g. "x.inlanefreight.htb"
<test>Brute-force vhosts on the target system. What is the full subdomain that is prefixed with "a"? Answer using the full domain, e.g. "x.inlanefreight.htb"
<test>Brute-force vhosts on the target system. What is the full subdomain that is prefixed with "su"? Answer using the full domain, e.g. "x.inlanefreight.htb"
\\============================================================
Certificate Transparency logs
at the heart of SSL/TLS lies the digital certificate, a small file that verifies a website's identity and allows for secures encrypted comm
- the process of issuing and managing these cert isn't footproof. atk can exploit rogue or mis-issued cer to impersonate legitimate web, intercept sensitive data, or spread malware. this is where certificate transparency (CT) logs come into play 
\\what are cert transparency logs?
it is public, append-only ledgers that record the issuance of SSL/TLS cert. whenever a cert authority(CA) issues a new cert
it must submit it to multi CT Logs indemendent organisations maintain these logs and are open for anyone to inspect
-it like global registry of certificates. they provide a transparent and verifiable record of every SSL/TLS certificate issued for a website. this transparency serves several crucial purposes 
    +Early Detection of Rogue Certificates: by monitoring CT logs ->ealy detect and revoke the cert
    +Accountability for Certificate Authorities: CT logs hold CAs accountable for their issuance practices
    +Strengthening the Web PKI (public key infrastructure): he web KPI is the trust system underpinning secure online comm. CTE log enhance the secu and integrity of the web PKI by providing a mechansm for public oversight and veri of cert
\\How cert transparency logs work
1. Certificate issuance + 2.log submission + 3.Signed Certificate timestamp(SCT) + 4. Browser verification
+5. Monitoring and Auditing
\\the Merkle Tree structure
this process ensure that any bit data change in a cert or log altered will cause the root hash change, immediately sinaling tampering. this makes CT logs an invaluable tool for maintaining the itegrity and trustworthiness of SSL/TLS certificate
\\CT Logs and Web reconn
-cert transparency logs offer a unique advantage in sub enumeration compared to other methods
-unlike brute-force or wordlist based approaches which rely on guessing or predicting subdomain names,
CT logs provide a definitive records of cert issued for a domain and its sub -> gain access to a historical and comprehensive view of a domain, including those that might not be actively used or easily guessable
-CT logs can unveil subdomains associated with old or expired cert. these sub might host outdated soft or configuration, making them potentially vulnerable to exploitation
-CT logs provide a reliable and efficient way to discover sub without the need for exhausive brute or relying on wordlist
-it offer a unique window into a domains history and can reveal subdomains that might otherwise remain hidden, significantly enhancing your recon capabilities
\\Searching CT Logs
2 popular options for searching CT Logs:
Tool	Key Features	Use Cases	Pros	Cons
$crt.sh	User-friendly web interface, simple search by domain, displays certificate details, SAN entries.	Quick and easy searches, identifying subdomains, checking certificate issuance history.	Free, easy to use, no registration required.	Limited filtering and analysis options.
$Censys	Powerful search engine for internet-connected devices, advanced filtering by domain, IP, certificate attributes.	In-depth analysis of certificates, identifying misconfigurations, finding related certificates and hosts.	Extensive data and filtering options, API access.	Requires registration (free tier available).
\\crt.sh lookup
leverage crt API for automated searches directly from terminal
find all 'dev'subdomains on facebook.com using curl and jq:
$curl -s "https://crt.sh/?q=facebook.com&output=json" | jq -r '.[] | select(.name_value | contains("dev)) | .name_value' | sort -u
curl -s "https://crt.sh/?q=facebook.com&output=json": This command fetches the JSON output from crt.sh for certificates matching the domain facebook.com.
jq -r '.[] | select(.name_value | contains("dev")) | .name_value': This part filters the JSON results, selecting only entries where the name_value field (which contains the domain or subdomain) includes the string "dev". The -r flag tells jq to output raw strings.
sort -u: This sorts the results alphabetically and removes duplicates
\\\\\\\============================================
Fingerprinting identity
+targed attack: tech, focus efforts on exploits and vuls, significantly increase chances
+identifying Misconfigurations:outdated soft, default settings, 
+Prioritising Targets: identifying which systems more likely vul, value
+building a comprehensive profile: understand its overal security posture and potential atk venctors
\\Fingerprinting techniques
+banner grabbing:it often reveal the svr software,version,..
+Analysing HTTP Header: typically discloses the web svr software, while the X-Powered-By header might reveal tech like scripting languages or framworks
+Probing for Specific Responses: certain error messages or behavior are characteristic of particular web svr or software components
+Analysing Page content:maybe copy right header that indicates specific software being used 
\\\Tools
+Wappalyzer +builtWith +WhatWeb +Nmap +Netcraft +wafw00f 
Tool	Description	Features
Wappalyzer	Browser extension and online service for website technology profiling.	Identifies a wide range of web technologies, including CMSs, frameworks, analytics tools, and more.
BuiltWith	Web technology profiler that provides detailed reports on a website's technology stack.	Offers both free and paid plans with varying levels of detail.
WhatWeb	Command-line tool for website fingerprinting.	Uses a vast database of signatures to identify various web technologies.
Nmap	Versatile network scanner that can be used for various reconnaissance tasks, including service and OS fingerprinting.	Can be used with scripts (NSE) to perform more specialised fingerprinting.
Netcraft	Offers a range of web security services, including website fingerprinting and security reporting.	Provides detailed reports on a website's technology, hosting provider, and security posture.
wafw00f	Command-line tool specifically designed for identifying Web Application Firewalls (WAFs).	Helps determine if a WAF is present and, if so, its type and configuration.

\\Pringerpriting inlanefreight.com
+banner grabbing
gather information directly from the web svr itself. we can do this using the curl command with the -I flag (or --head) to fetch only the HTTP headers, not the entire page content.
$curl -I inlanefreight.com
-the output will include the svr banner, reveal the web svr software and version
$curl -I inlanefreight.com
\\Wafw00f 
before proceeding with further fingerprinting. it's crucial to determine if inlanefreight.com employs a WAF, as it could interfere with our probes or potentially block our requests.
-install wafw00f $pip3 install git+https://github.com/EnableSecurity/wafw00f
$wafw00f <url>
\\Nikto
in additional to its primary function as a vul assessment tool, Nikto's fingerprint capabilities provide insights into a website's tec stack
-install nikto $sudo apt update && sudo apt install -y perl 
-git clone https://github.com/sullo/nikto
-cd nikto/program
-chmod +x ./nikto.plans
$nikto -h inlanefreight.com -Tuning b 
<exp> -h flag specifies the target host/ the -Tuning b flag tells Nikto to only run the software identification modules.
-Nikto then initiate a series of tests. attempting to identify outdated software, insecure files or configurations and other potential security risks
$nikto -h inlanefreight.com -Tuning b 
<test>app.inlanefreight.local
dev.inlanefreight.local 10.129.52.176
<test> Determine the Apache version running on app.inlanefreight.local on the target system. (Format: 0.0.0)
$nikto -h inlanefreight.com -Tuning b 
<test>Which CMS is used on app.inlanefreight.local on the target system? Respond with the name only, e.g., WordPress.
utilize Wappalyser 
curl -s http://app.inlanefreight.local/index.php | grep '<meta name="generator"'
 On which operating system is the dev.inlanefreight.local webserver running in the target system? Respond with the name only, e.g., Debian.
\\\\\\\\\\\\\\\==========================================
Crawling/spidering, is automated process of systematically browsing the world wide web
bots that use pre-defined algo to discover and index web pages.
\\how it work 
start with seed URL,  initial web page to crawl. the crawler fetches this page, parses its content, and extracts all its links
-> then add these links to a queue and crawls them, repeating the process iteratively. -> it can crawler can explore an entire web
crawler discovers and collects infor by systematically following links. != it from fuzzing which involves guessing potential links
\\Breadth-first crawling exploring a web width before going deep.
\\Depth-first crawling prioritizes depth over breadth. follows a single path of link 
\\Extracting Valuable Information it can extract a diverse array of data, each serving a specific purpose in the reconnaissance process
+link (internal and external) map out structer, discover hidden links, external regulation
+comments. reavel sensitive detail, internal processes or hints of vul in their comments
+Metadata: refer to data about data. in web page it includes information like page titles, descriptions, keywords, author names, and dates. this metadata can provide valuable context about a page's contents, purpose and relevance to golang
+sensitive files: actively search for sensitive files that might be inadvertently exposed on a web. this includes backup files (e.g,. bak, .old), configuration files (web.config, setting.php), log files (error_log, access_log) other file pwd, API keys, or other confidential infor.
\\The importance of context
\\\\\\=====================================================
robots.txt
it act as a virtual "etiquette guide" for bots. outlining which areas of a web they are allowed to access and which are off-limit
\\What is robots.txt
it placed in the root dir of a web (ex ww.example.com/robots/txt) in adheres to the robots exclusion standard. guidelines for how web crawlers should behave when visiting a web. this file contains instructions in form of "directories" that tell bots which parts of the website they can and cannot crawl
\\How robots.txt work
typically target specific user-agents. which are identifiers for diff types of bots.
User-agent: *
Disallow: /private/
->this directive tells all user-agent (* is wildcard) that they are not allowed to access any URLs that start with
/private/. other dir can allow access to specific dir or files. set crawl delays to avoid overloading a server or provide links to sitemapps for efficient crawling
\\Understand robots.txt structure
1.User-agent (*): specifies which crawler or bot the following rules apply to.it apply to all bots.
2. Directives:these lines provide specific instructions to the identified user-agent

Directive	Description	Example
Disallow	Specifies paths or patterns that the bot should not crawl.	Disallow: /admin/ (disallow access to the admin directory)
Allow	Explicitly permits the bot to crawl specific paths or patterns, even if they fall under a broader Disallow rule.	Allow: /public/ (allow access to the public directory)
Crawl-delay	Sets a delay (in seconds) between successive requests from the bot to avoid overloading the server.	Crawl-delay: 10 (10-second delay between requests)
Sitemap	Provides the URL to an XML sitemap for more efficient crawling.	Sitemap: https://www.example.com/sitemap.xml
\\Why Repect rebots.txt
while robots.txt is not strictly enforceable (a rogue bot could still ignore it) most legitimate web crawlers and serach engine bots will repsect its directives.
+Avoiding overburdening servers
+protect sensitive information
\\robots.txt in web recon
rebots.txt serves as a valuable source of intelligence. while respecting the dir outlined in this file.
+uncovering hidden directories: 
+mapping website structure
+detecting crawler traps: some web intentionally include honeyport dir in robots.txt to lure malicious bots. identifying such traps can provide insight into the target security awareness and defensive measures
\\Analyzing robots.txt
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/

User-agent: Googlebot
Crawl-delay: 10

Sitemap: https://www.example.com/sitemap.xml
\\Well-known URLs
the .well-known standard, defined in RFC 9615, serves as a standardized directory within a web root domain
/.well-known/ centralizes a web critical metadata, including configuration files and information related to its services, protocols, and security mechanisms
-establishing a consistent location for such data, .well-known simplifies the discovery and access process for various stakeholders, inclu web browser, app, and security tools.
-this streamlined approach enables clients to auto locate and retrieve specific config files by constructing the appropriate URL 
EX: to access a web security policy https://example.com/.well-known/security.txt
-the internet assigned numbers authority (IANA) maintains a registry of .well-known URLs, each serving aa specific purpose defined by various specifications and standard.
+  security.txt  +/.well-known/change-password +openid-configuration +assetlinks.json 
+mta-sts.txt 
\\Web Recon and .well-known
discovering endpoints and config details. particularly useful is openid-configuration.
part of the openID connect discovery protocol, an identity layer build on top of the OAuth 2.0 protocol
-when a client app wants to use OpenID connect for authen, it can retrieve the OpenID Connect Providers config by access
https://example.com/.well-known/openid-configuration endpoint. it return a JSON doc contain metadata such as authen methods, token issuance
{
  "issuer": "https://example.com",
  "authorization_endpoint": "https://example.com/oauth2/authorize",
  "token_endpoint": "https://example.com/oauth2/token",
  "userinfo_endpoint": "https://example.com/oauth2/userinfo",
  "jwks_uri": "https://example.com/oauth2/jwks",
  "response_types_supported": ["code", "token", "id_token"],
  "subject_types_supported": ["public"],
  "id_token_signing_alg_values_supported": ["RS256"],
  "scopes_supported": ["openid", "profile", "email"]
}
=>1.Endpoint Discovery; +author endpoint +token endpoint + userinfo Endpoint
2.JWKS URI , the jwks_uri reveals the JSON web key set detailing the cryptographic keys used by the server
3. Supported scopes and response types:understanding which scopes and response types are supported helps in mapping out the functionality and limitations of the openID connect implementation
4. Algorithm Details:information about supported signing algorithms can be crucial for understanding the security measures in placed
detail in https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml
\\\\\\\\\\\\\========================================================
Creepy Crawlies
plethora of web crawling tools each with its own strenghs and specialties.allow analyzing the extracted data
\\Popular Web crawlers
+Burp Suite Spider: identifying hidden content, and uncovering potential vul
+OWASP ZAP (Zed Attack Proxy): free crawl web app and identify vul
+Scrapy (python framwork): versatile and scalable python framwork for building custom web crawlers. extract structured data, handling complex crawling scenarios, automate data processing, flexibility make it ideal for tailored recon task
+Apache Nutch (Scalable Crawler): handle massive crawls across the entire web or focus on specific domains, it requires more technical expertise to setup and configure, its power and flexibility make it a valuable asset for large-scale recon project
=> adhering to ethical and responsible crawling, avoid overloading with excessive requests
\\Scrapy
\\installing Scrapy
$pip3 install Scrapy
\\ReconSpider
$wget -O ReconSpider.zip  https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip
$unzip ReconSpider.zip
$python3 ReconSpider.py http://inlanefreigh.com
=> it will result in JSON file
SON Key	Description
emails	Lists email addresses found on the domain.
links	Lists URLs of links found within the domain.
external_files	Lists URLs of external files such as PDFs.
js_files	Lists URLs of JavaScript files used by the website.
form_fields	Lists form fields found on the domain (empty in this example).
images	Lists URLs of images found on the domain.
videos	Lists URLs of videos found on the domain (empty in this example).
audio	Lists URLs of audio files found on the domain (empty in this example).
comments	Lists HTML comments found in the source code.
